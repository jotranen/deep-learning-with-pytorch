{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1f61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180d9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2961b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation split\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    " \n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d4c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1382643] [0.49671415]\n",
      "[1.02354084] [1.9689643]\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Initialize parameters randomly\n",
    "np.random.seed(42)\n",
    "w = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(b, w)\n",
    "\n",
    "# Set learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Define epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1: Compute predictions\n",
    "    y_pred = w * x_train + b\n",
    "\n",
    "    # Step 2: Compute loss\n",
    "    error = (y_pred - y_train)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3: Compute gradients\n",
    "    grad_b = 2 * error.mean()\n",
    "    grad_w = 2 * (error * x_train).mean()\n",
    "\n",
    "    # Step 4: Update parameters\n",
    "    b = b - lr * grad_b\n",
    "    w = w - lr * grad_w\n",
    "\n",
    "print(b,w)\n",
    "    # Step 5: Print progress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c657239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "\n",
    "print(linr.intercept_, linr.coef_[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb6b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cuda\" if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ff73d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chipset Model: Apple M3 Pro\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "gpu_info = subprocess.run(\n",
    "    [\"system_profiler\", \"SPDisplaysDataType\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ").stdout\n",
    "\n",
    "for line in gpu_info.splitlines():\n",
    "    if \"Chipset Model\" in line or \"Chip\" in line:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0646b98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7713], device='mps:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_float = x_train.astype(np.float32)\n",
    "gpu_tensor = torch.as_tensor(x_train_float).to(device)\n",
    "\n",
    "gpu_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ccf126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9047], device='mps:0', requires_grad=True) tensor([-1.5057], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(b, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "743bb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5973125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='mps:0', requires_grad=True) tensor([1.9690], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "\n",
    "    # Step 2 - Computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient\n",
    "    # descent. How wrong is our model? That's the error!\n",
    "    error = (yhat - y_train_tensor)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    # make_dot(yhat)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\"\n",
    "    # parameters. No more manual computation of gradients!\n",
    "    # b_grad = 2 * error.mean()\n",
    "    # w_grad = 2 * (x_tensor * error).mean()\n",
    "    # We just tell PyTorch to work its way BACKWARDS\n",
    "    # from the specified\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate. But not so fast...\n",
    "    # FIRST ATTEMPT - just using the same code as before\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # b = b - lr * b.grad                          ①\n",
    "    # w = w - lr * w.grad                          ①\n",
    "    # print(b)                                     ①\n",
    " \n",
    "    # SECOND ATTEMPT - using in-place Python assignment\n",
    "    # RuntimeError: a leaf Variable that requires grad\n",
    "    # has been used in an in-place operation.\n",
    "    # b -= lr * b.grad                             ②\n",
    "    # w -= lr * w.grad                             ②\n",
    " \n",
    "    # THIRD ATTEMPT - NO_GRAD for the win!\n",
    "    # We need to use NO_GRAD to keep the update out of\n",
    "    # the gradient computation. Why is that? It boils\n",
    "    # down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():                          \n",
    "        b -= lr * b.grad                           \n",
    "        w -= lr * w.grad                           \n",
    " \n",
    "    # PyTorch is \"clingy\" to its computed gradients; we\n",
    "    # need to tell it to let it go...\n",
    "    b.grad.zero_()                                 \n",
    "    w.grad.zero_()                                 \n",
    " \n",
    "print(b, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709540f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 13.1.2 (20250808.2320)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"220pt\" height=\"292pt\"\n",
       " viewBox=\"0.00 0.00 220.00 292.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 287.75)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-287.75 216,-287.75 216,4 -4,4\"/>\n",
       "<!-- 6043526800 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>6043526800</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"138,-32.75 74,-32.75 74,0 138,0 138,-32.75\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"106\" y=\"-7.25\" font-family=\"monospace\" font-size=\"10.00\"> (80, 1)</text>\n",
       "</g>\n",
       "<!-- 6043605152 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>6043605152</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"150,-89.5 62,-89.5 62,-68.75 150,-68.75 150,-89.5\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"106\" y=\"-76\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 6043605152&#45;&gt;6043526800 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6043605152&#45;&gt;6043526800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-68.36C106,-61.89 106,-53.05 106,-44.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-44.55 106,-34.55 102.5,-44.55 109.5,-44.55\"/>\n",
       "</g>\n",
       "<!-- 6043613888 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>6043613888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-146.25 0,-146.25 0,-125.5 100,-125.5 100,-146.25\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"50\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6043613888&#45;&gt;6043605152 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>6043613888&#45;&gt;6043605152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.01,-125.09C67.74,-117.53 78.63,-106.89 87.87,-97.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.29,-100.38 94.99,-90.89 85.4,-95.37 90.29,-100.38\"/>\n",
       "</g>\n",
       "<!-- 6043528240 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>6043528240</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77,-215 23,-215 23,-182.25 77,-182.25 77,-215\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"50\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 6043528240&#45;&gt;6043613888 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>6043528240&#45;&gt;6043613888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-181.98C50,-174.6 50,-165.68 50,-157.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-157.91 50,-147.91 46.5,-157.91 53.5,-157.91\"/>\n",
       "</g>\n",
       "<!-- 6043610960 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>6043610960</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"206,-146.25 118,-146.25 118,-125.5 206,-125.5 206,-146.25\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"162\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 6043610960&#45;&gt;6043605152 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>6043610960&#45;&gt;6043605152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.99,-125.09C144.26,-117.53 133.37,-106.89 124.13,-97.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"126.6,-95.37 117.01,-90.89 121.71,-100.38 126.6,-95.37\"/>\n",
       "</g>\n",
       "<!-- 6043607840 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6043607840</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"212,-209 112,-209 112,-188.25 212,-188.25 212,-209\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"162\" y=\"-195.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6043607840&#45;&gt;6043610960 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6043607840&#45;&gt;6043610960</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162,-187.86C162,-179.76 162,-167.94 162,-157.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.5,-157.92 162,-147.92 158.5,-157.92 165.5,-157.92\"/>\n",
       "</g>\n",
       "<!-- 6043527952 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6043527952</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"189,-283.75 135,-283.75 135,-251 189,-251 189,-283.75\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"162\" y=\"-258.25\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 6043527952&#45;&gt;6043607840 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>6043527952&#45;&gt;6043607840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162,-250.56C162,-241.66 162,-230.43 162,-220.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.5,-220.85 162,-210.85 158.5,-220.85 165.5,-220.85\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x123b91410>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "\n",
    "yhat = b + w * x_train_tensor\n",
    "# Step 2 - Computes the loss\n",
    "error = (yhat - y_train_tensor)\n",
    "loss = (error ** 2).mean()\n",
    "# We can try plotting the graph for any variable: yhat, error, loss\n",
    "make_dot(yhat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28d4fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0235], device='mps:0', requires_grad=True) tensor([1.9690], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "\n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    # make_dot(yhat)\n",
    "\n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\"\n",
    "    # parameters. No more manual computation of gradients!\n",
    "    # b_grad = 2 * error.mean()\n",
    "    # w_grad = 2 * (x_tensor * error).mean()\n",
    "    # We just tell PyTorch to work its way BACKWARDS\n",
    "    # from the specified\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate. But not so fast...\n",
    "    optimizer.step()                          \n",
    " \n",
    "    # PyTorch is \"clingy\" to its computed gradients; we\n",
    "    # need to tell it to let it go...\n",
    "    optimizer.zero_grad()                                 \n",
    " \n",
    "print(b, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e076f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"b\" and \"w\" real parameters of the model,\n",
    "        # we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(torch.randn(1,\n",
    "                                          requires_grad=True,\n",
    "                                          dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1,\n",
    "                                          requires_grad=True,\n",
    "                                          dtype=torch.float))\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "dummy = ManualLinearRegression()\n",
    "list(dummy.parameters())\n",
    "\n",
    "dummy.state_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5bbc9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.1,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'differentiable': False,\n",
       "   'fused': None,\n",
       "   'params': [0, 1]}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a97d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([1.0235], device='mps:0')), ('w', tensor([1.9690], device='mps:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like\n",
    "# Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)       \n",
    " \n",
    "# Defines an SGD optimizer to update the parameters\n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    " \n",
    "# Defines an MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # What is this?!?                \n",
    " \n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)                   \n",
    " \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    " \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    " \n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    " \n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b45d658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.3013,  0.3034,  0.2159],\n",
       "                      [-0.1015, -0.1529,  0.0618],\n",
       "                      [-0.1020, -0.1721,  0.3690],\n",
       "                      [ 0.4962, -0.0572, -0.1293],\n",
       "                      [ 0.0084, -0.0345,  0.1388]], device='mps:0')),\n",
       "             ('0.bias',\n",
       "              tensor([ 0.1618, -0.5244, -0.2131,  0.4862,  0.2249], device='mps:0'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(3, 5)).to(device)\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0175cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0287, -0.3481, -0.3532],\n",
       "                      [-0.5172, -0.1882,  0.1950],\n",
       "                      [ 0.3681,  0.2666, -0.5103],\n",
       "                      [-0.3472, -0.0911,  0.5585],\n",
       "                      [ 0.0835, -0.1495,  0.2389]], device='mps:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.2199, -0.3737,  0.4214, -0.2625, -0.1157], device='mps:0')),\n",
       "             ('1.weight',\n",
       "              tensor([[-0.4449,  0.2993,  0.3388,  0.1630, -0.3118]], device='mps:0')),\n",
       "             ('1.bias', tensor([-0.4414], device='mps:0'))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(3, 5), nn.Linear(5, 1)).to(device)\n",
    "\n",
    "model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71dc883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_preparation/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_preparation/v0.py\n",
    " \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    " \n",
    "# Our data was in Numpy arrays, but we need to transform them\n",
    "# into PyTorch's Tensors and then send them to the\n",
    "# chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c08a5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparation/v0.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "329ebdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_configuration/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v0.py\n",
    " \n",
    "# This is redundant now, but it won't be when we introduce\n",
    "# Datasets...\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    " \n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    " \n",
    "torch.manual_seed(42)\n",
    " \n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    " \n",
    "# Defines an SGD optimizer to update the parameters\n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    " \n",
    "# Defines an MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23ff8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_training/v0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_training/v0.py\n",
    " \n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sets model to TRAIN mode\n",
    "    model.train()\n",
    " \n",
    "    # Step 1 - Computes model's predicted output - forward pass\n",
    "    yhat = model(x_train_tensor)\n",
    " \n",
    "    # Step 2 - Computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    " \n",
    "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4 - Updates parameters using gradients and\n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
